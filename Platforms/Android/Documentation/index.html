---
layout: default
---
<h1>Affdex SDK for Android</h1>

<p><p></p>

<h2>Developer Guide
For SDK version 1.1</h2>

<p><p></p>

<h2>Introduction<h2>
<p>
The Affdex SDK is the culmination of years of scientific research into emotion detection, validated across thousands of tests worldwide on PC platforms, and now made available on Android and Apple iOS. Affdex SDK turns your ordinary app into an extraordinary app by emotion-enabling your app to respond in real-time to user emotions.
In this document, you will become familiar with integrating the Affdex SDK into your Android app. Please take time to read this document and feel free to give us feedback at sdk@affectiva.com.
<p><h2>What’s in the SDK</h2>

<p>The Affdex SDK package consists of the following:
<ul>
<li>   Introducing the SDK.pdf <br>
</li><li>   SDK Developer Guide.pdf (this document) <br>
</li><li>   docs, the folder containing documentation. In the javadoc subfolder, start with index.html. <br>
</li><li>   libs, the folder containing Affdex SDK libraries that your app will link against assets, the folder containing files needed by the SDK <br>
</li></ul>
<p><h2>Requirements</h2>
The Affdex SDK requires a device running Android API 16 or above.
Java 1.6 or above is required on your development machine.
The SDK requires access to external storage on the Android device, and Internet access for collecting anonymous analytics (see “A Note about Privacy” in “Introducing the SDK”). Include the following in your app’s AndroidManifest.xml:
    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /> <br>
    <uses-permission android:name="android.permission.INTERNET" />
    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE"/>
<p><h2>Licensing</h2>
After you request the SDK, Affectiva will provide to you an Affectiva license file.  Copy this file into your Android app project under the folder /assets/Affdex, and specify its relative path under that folder when invoking the setLicensePath method (described in more detail below).
<p><h2>Outline</h2>
This document will guide you through the following:
<ul><li>   Adding the SDK to your Android project <br>
</li><li>   Using the SDK <br>
</li><li>   Options <br>
</li><li>   Interpreting the data <br>
</li><li>   A note about SDK analytics (Flurry) <br>
</li><li>   Where to Go From Here <br>
</li></ul>
<p><h2>Add the SDK to your Project</h2>
In order to use this SDK in one of your Android apps, you will need to copy some files from the SDK into your Android project. In your Android project, alongside your “src and “res” folders, you may have the optional folders “assets” and “libs”. Copy the SDK’s “assets” folder into your project. If you already have an “assets” folder, copy the contents of the SDK’s “assets” folder into your “assets” folder. In a similar way, copy the SDK's “libs” folder into your project. <br>
The provided sample app does not have “assets” or “libs” folders.  In this case, simply copy the entire “assets and “libs” folders into the app’s project, at the same level as the “res” and “src” folders.
We do not recommend adding any of your own files to the “assets/Affdex” folder.
<p><h2>Using the SDK</h2>
The following code snippets demonstrate how easy it is to obtain facial expression results using your device’s camera, a video file, or from images.
<p><h2>SDK Operating Modes</h2>
The Affdex SDK has the following operating modes:
<ul><li>   Camera mode:  the SDK turns connects to the camera and processes the frames it records.  Sample app: MeasureUp. <br>
</li><li>   Video file mode:  provide to the SDK a path to a video file. <br>
</li><li>   Pushed frame mode:  provide to the SDK individual frames of video and their timestamps. <br>
</li><li>   Photo mode:  provide discrete images to the SDK (unrelated to any other image).
</li></ul>
The SDK provides mode-specific Detector classes for each of these modes: CameraDetector, VideoFileDetector, FrameDetector, and PhotoDetector. <br>
<p><h2>SDK calling overview</h2>
In general, calls to the SDK are made in the following order:
<ul><li>   Construct a Detector corresponding to the operating mode that you want. The following methods are called on a Detector instance. <br>
</li><li>   Call setLicensePath() with the path to the license file provided by Affectiva. <br>
</li><li>   Set options for the Detector. In particular, enable detection of at least one facial expression metric (e.g. call setDetectSmile() to detect smiles).  Several facial expressions can be detected by the SDK, as described in “Introducing the SDK”. See the “Options” section below for more information on the different options available. <br>
</li><li>   Call start() to start processing.  Note the types of exceptions that can be thrown and handle them as desired. <br>
</li><li>   If you are pushing your own images (pushed frame mode or photo mode), call process() with each image. <br>
</li><li>   When you are done processing, call stop(). <br>
</li></ul>
<p>To receive results from the SDK, implement the Detector.ImageListener and/or Detector.FaceListener interfaces, and register your listener object(s) with the Detector via setImageListener() and/or setFaceListener(). These interfaces provide results of the SDK’s processing of each frame.  The ImageListener interface provides information about facial expressions and face points for a face found in a given image via its onImageResults callback.  The FaceListener interface notifies its listener when a face appears or disappears via its onFaceDetectionStarted() and onFaceDetectionStopped() callbacks. For an example of using these callbacks to show and hide the results from the SDK, see the sample app MeasureUp. <br>
To check to see if the Detector is running (start() has been called, but not stop()), call isRunning(). <br>
<p>Note: Be sure to always call stop() following a successful call to start() (including for example, in circumstances where you abort processing, such as in exception catch blocks).  This ensures that resources held by the Detector instance are released.</p>

<h2>Camera Mode</h2>

<p>Using the built-in camera is a common way to obtain video for facial expression detection. Either the front or back camera of your Android device can be used to feed video directly to the Detector. <br>
A demonstration of Camera Mode is the sample app MeasureUp. <br>
To use Camera Mode,  implement the Detector.ImageListener and/or Detector.FaceListener interface. Then follow this sequence of SDK calls: <br>
<ul><li>   Construct a CameraDetector. The cameraType argument specifies whether to connect to the front or back camera, while the cameraPreviewView argument optionally specifics a SurfaceView onto which the SDK should display preview frames (specify null for this argument if you don’t care about previewing the frames).
    public CameraDetector(Context context, CameraType cameraType,
                   SurfaceView cameraPreviewView) <br>
</li><li>   Call setLicensePath() with the path to the license file provided by Affectiva. <br>
</li><li>   Set options for the Detector. In particular, turn on at least one facial expression metric to detect facial expressions, e.g.
setDetectSmile(true); <br>
</li><li>   Call start() to start processing.  Note that if the camera is already in use, an exception will be thrown. If successful, you will start receiving calls to onImageResults(). <br>
</li><li>   When you are done, call stop(). <br>
</li></ul>
<p><h2>Video File Mode</h2>
Another way to feed video into the detector is via a video file that is stored on the file system of your device. Follow this sequence of SDK calls: <br>
<ul><li>   Construct a VideoFileDetector. The filePath argument is the path to your video file.
public Detector(Context context, String filePath) <br>
</li><li>   Call setLicensePath() with the path to the license file provided by Affectiva. <br>
</li><li>   Set options for the Detector. In particular, turn on at least one facial expression metric to detect facial expressions, e.g. <br>
setDetectSmile(true); <br>
</li><li>   Call start() to start processing. You will start receiving calls to onImageResults(). <br>
</li><li>   When you are done processing, call stop(). <br>
</li></ul>
<p><h2>Pushed Frame Mode</h2>
If your app is processing video and has access to video frames, you can push those video frames to the Affdex SDK for processing.  Each video frame has an associated timestamp that increases with each frame in the video.  Your app may have access to video frames because your app is interfacing to the device’s camera, or because your app is reading a video file, or perhaps by some other method. <br>
<ul><li>   Construct a FrameDetector.
public FrameDetector(Context context) <br>
</li><li>   Call setLicensePath() with the path to the license file provided by Affectiva. <br>
</li><li>   Set options for the Detector. In particular, turn on at least one facial expression metric to detect facial expressions, e.g. <br>
setDetectSmile(true); <br>
</li><li>   Call start() to start processing. <br>
</li><li>   For each video frame, create an Affdex Frame (Bitmap, RGBA, and YUV420sp/NV21 formats are supported).  Note: Frame is an abstract base class with two concrete subclasses: BitmapFrame and ByteArrayFrame; you should construct one of these concrete subclasses. <br>
</li><li>   Call process with the Affdex Frame and timestamp of the frame:
public abstract void process(Frame frame, float timestamp); <br>
</li><li>   For each call to process, the SDK will call onImageResults(). <br>
</li><li>   When you are done processing, call stop(). <br>
</li></ul>
<p><h2>Photo Mode</h2>
Use Photo Mode for processing images that are unrelated to each other (that is, they are not sequential frames of a video). Discrete images are processed by the SDK independently, without regard to the content of the preceding images, using different algorithms and data than are used with the other modes involving sequences of frames from a video source. <br>
<ul><li>   Construct a PhotoDetector. <br>
public Detector(Context context) <br>
</li><li>   Call setLicensePath() with the path to the license file provided by Affectiva. <br>
</li><li>   Set options for the Detector. In particular, turn on at least one facial expression metric to detect facial expressions, e.g. <br>
setDetectSmile(true); <br>
</li><li>   Call start() to initialize the PhotoDetector. <br>
</li><li>   For each photo to be processed, create an Affdex Frame from your frame (Bitmap, RGBA, and YUV420sp/NV21 formats are supported). Note: Frame is an abstract base class with two concrete subclasses: BitmapFrame and ByteArrayFrame; you should construct one of these concrete subclasses. <br>
</li><li>   Call process with the Affdex Frame: <br>
public abstract void process(Frame frame); <br>
</li><li>   For each call to process, the SDK will call onImageResults(). <br>
</li><li>   When you are done processing, call stop(). <br>
</li></ul>
<p><h1>Options</h1>
This section describes various options for operating the SDK. <br>
<p><h2>Detecting facial expressions</h2>
The Affdex SDK can detect a variety of facial expressions, yielding metric scores for the expressions you configure.  Affdex expression metrics are described in detail in the “Introducing the SDK” document. By default, no expressions are detected. Detection can be enabled or disabled via setDetectXXX methods defined on the Detector class.  For example: <br>
setDetectSmile(true) <br>
See the Detector class Javadoc for a complete list of the methods available.
<p><h2>Processing Rate</h2>
In Camera Mode, you can specify the maximum number of frames per second that the SDK should process. This can improve performance if your requirements do not require every frame in the video stream from the camera to be processed. The default (and recommended) rate is 5 frames per second, but you may also set it lower if you are using a slower device, and need additional performance. Here is an example of setting the processing rate to 2 FPS: <br>
setMaxProcessRate(2); <br>
<p><h2>Face Detection Statistics</h2> <br>
To get the percentage of time a face was detected during a run (between start() and stop()), call: <br>
    getPercentFaceDetected(); <br>
This can only be called after stop(). <br>
<p><h2>Interpreting the Data</h2>
To receive the results of the SDK’s processing of a frame, implement the Detector.ImageListener and/or Detector.FaceListener interfaces.  </p>

<p>For the ImageListener interface, implement the callback onImageResults(), which is called by the SDK for every frame (except those that the CameraDetector skips in order to honor the maximum processing rate, unless setSendUnprocessFrames(true) has been called). <br>
This method receives these parameters: <br>
<ol><li>  A list of Face objects.  In this release, this will be an empty list if no face was found in the frame, or a list of one Face object if there was a face found in the frame.  In Camera Mode, if setSendUnprocessedFrames(true) has been called, then this parameter will be null for any frame that has been skipped in order to honor the maximimum processing rate. <br>
</li><li>  The image processed, as an Affdex Frame (a wrapper type for images, including Bitmaps, for example). <br>
</li><li>  The timestamp of the frame.  In Photo Mode, this will be zero.
</li></ol>
The returned Face object provides getter methods for retrieving facial expression scores corresponding to the expressions previously configured on the Detector.  Scores are generally values from 0-100, representing the expression as a percent, with the exception of “valence” which ranges between -100 and +100.  </p>

<p><p><h2>Face Point indices</h2>
The indices of the elements in the face points array correspond to specific locations on a face.  Please see the table below for an explanation of the locations corresponding to each index.  </p>

<p>
<table style="width:100%">
<tr>
<th> Index </th>
<th> Point on face </th>
<th> Index </th>
<th> Point on face </th>
</tr>
<tr>
<td> 0 </td>
<td> Right Top Jaw </td>
<td> 17 </td>
<td> Inner Right Eye </td>
</tr>
<tr>
<td> 1 </td>
<td> Right Jaw Angle </td>
<td> 18 </td>
<td> Inner Left Eye </td>
</tr>
<tr>
<td> 2 </td>
<td> Gnathion </td>
<td> 19 </td>
<td> Outer Left Eye </td>
</tr>
<tr>
<td> 3 </td>
<td> Left Jaw Angle </td>
<td> 20 </td>
<td> Right Lip Corner </td>
</tr>
<tr>
<td> 4 </td>
<td> Left Top Jaw </td>
<td> 21 </td>
<td> Right Apex Upper Lip </td>
</tr>
<tr>
<td> 5 </td>
<td> Outer Right Brow Corner </td>
<td> 22 </td>
<td> Upper Lip Center </td>
</tr>
<tr>
<td> 6 </td>
<td> Right Brow Center </td>
<td> 23 </td>
<td> Left Apex Upper Lip </td>
</tr>
<tr>
<td> 7 </td>
<td> Inner Right Brow Corner </td>
<td> 24 </td>
<td> Left Lip Corner </td>
</tr>
<tr>
<td> 8 </td>
<td> Inner Left Brow Corner </td>
<td> 25 </td>
<td> Left Edge Lower Lip </td>
</tr>
<tr>
<td> 9 </td>
<td> Left Brow Center </td>
<td> 26 </td>
<td> Lower Lip Center </td>
</tr>
<tr>
<td> 10 </td>
<td> Outer Left Brow Corner </td>
<td> 27 </td>
<td> Right Edge Lower Lip </td>
</tr>
<tr>
<td> 11 </td>
<td> Nose Root </td>
<td> 28 </td>
<td> Bottom Upper Lip </td>
</tr>
<tr>
<td> 12 </td>
<td> Nose Tip </td>
<td> 29 </td>
<td> Top Lower Lip </td>
</tr>
<tr>
<td> 13 </td>
<td> Nose Lower Right Boundary </td>
<td> 30 </td>
<td> Upper Corner Right Eye </td>
</tr>
<tr>
<td> 14 </td>
<td> Nose Bottom Boundary </td>
<td> 31 </td>
<td> Lower Corner Right Eye </td>
</tr>
<tr>
<td> 15 </td>
<td> Nose Lower Left Boundary </td>
<td> 32 </td>
<td> Upper Corner Left Eye </td>
</tr>
<tr>
<td> 16 </td>
<td> Outer Right Eye </td>
<td> 33 </td>
<td> Lower Corner Left Eye </td>
</tr>
<p>
Index | Point on face | Index   | Point on face
---|---|---|---
0   | Right Top Jaw | 17 |  Inner Right Eye
1   | Right Jaw Angle | 18 |    Inner Left Eye
2   | Gnathion  | 19 |  Outer Left Eye
3   | Left Jaw Angle |  20 |    Right Lip Corner
4   | Left Top Jaw  | 21 |  Right Apex Upper Lip
5   | Outer Right Brow Corner |     22 |    Upper Lip Center
6   | Right Brow Center | 23 |  Left Apex Upper Lip
7   | Inner Right Brow Corner | 24 |    Left Lip Corner
8   | Inner Left Brow Corner |  25 |    Left Edge Lower Lip
9   | Left Brow Center |    26 |    Lower Lip Center
10  | Outer Left Brow Corner |  27 |    Right Edge Lower Lip
11  | Nose Root |   28 |    Bottom Upper Lip
12  | Nose Tip |    29 |    Top Lower Lip
13  | Nose Lower Right Boundary |   30 |    Upper Corner Right Eye
14  | Nose Bottom Boundary |    31 |    Lower Corner Right Eye
15  | Nose Lower Left Boundary |        32 |    Upper Corner Left Eye
16  | Outer Right Eye | 33 |    Lower Corner Left Eye

<p><h1>Reference documentation</h1>
The SDK comes with detailed Javadoc in both JAR and HTML formats, describing all of the SDK’s classes and methods. <br>
<p><h2>Viewing the Javadoc in a browser: </h2>
Open the file docs/javadoc/index.html in the location where you installed the SDK. <br>
Viewing the Javadoc in your IDE:
<p><h3>Eclipse:</h3>
In your project’s libs folder, create a file called Affdex-sdk-1.1.jar.properties.  Edit that file in a text editor and enter a line like “doc=path/to/the/html/javadoc”.  The path specified should point to the docs/javadoc folder in your SDK installation folder, and can be an absolute or relative path. On Windows, use double backslashes to separate the folder names. <br>
<p><h3>Android Studio:</h3>
At the time of this writing, Android Studio does not yet support attaching javadoc to library dependencies. <br>
<p><h4>Getting started with the MeasureUp sample app</h4>
The SDK comes with a sample application called MeasureUp which demonstrates how to integrate the SDK into an app.  In this section, we’ll walk through the steps to build this app. <br>
Step 1: Download the MeasureUp sample app <br>
In a browser, open http://affdex-sdist.s3.amazonaws.com/Android/AndroidMeasureUp-1.1.zip, save it locally, and unzip it to wherever you normally put your Android projects.
Step 2: Copy assets and libraries packaged with the SDK into the MeasureUp project
Copy the “libs” and “assets” folders from the Affdex SDK installation folder to the folder where you unzipped the MeasureUp project. <br>
Step 3: Import the MeasureUp sample project into your IDE <br>
<p><h3>Eclipse:</h3>
<ul><li>   File->Import, choose General->Existing Projects into Workspace, click Next <br>
</li><li>   Browse to and select the folder where you unzipped the MeasureUp project, then click Finish <br>
</li></ul>
Note: you will see an error related to an unresolved resource ('@integer/google<em>play</em>services<em>version') in AndroidManifest.xml, which we will resolve in the next step. <br>
<p><h2>Android Studio:</h2>
<ul><li>   File->Import Project <br>
</li><li>   Browse to and select the folder where you unzipped the MeasureUp project, then click OK. <br>
</li><li>   On the Import Project from ADT (EclipseAndroid) dialog, specify a folder for the imported project location, and click Next, then click Finish.
</li></ul>
Note: you will see an error related to an unresolved resource ('@integer/google</em>play<em>services</em>version') in AndroidManifest.xml, which we will resolve in the next step.  </p>

<p>Step 4: Add project dependences for Google Play Services and Android v4 Support:
Projects using the SDK need to include two additional libraries that are packaged with the Android SDK: the Google Play Services library and the Android v4 Support library. <br>
To add the Google Play Services library to the project, follow instructions at: http://developer.android.com/google/play-services/setup.html
To add the Android v4 Support library to the project, follow the instructions at: https://developer.android.com/tools/support-library/setup.html <br>
Step 5: Add your license file to the project <br>
<ul><li>   Copy your Affectiva-provided license file to your project’s assets/Affdex folder. <br>
</li><li>   In your IDE, edit the source file MainActivity.java, and in the onCreate method, edit the following line to refer to your license file: <br>
        detector.setLicensePath("Affectiva.license");   </p>
</li></ul>
<p><p>That’s it!  You should now be able to build and run the MeasureUp app.  </p>

<h2>A Note about SDK Analytics (Flurry)</h2>

<p>The Affdex SDK for Android, and therefore by extension, any application that uses it, leverages the Flurry Analytics service to log events.  Due to a limitation in Flurry, an app cannot have two Flurry sessions open simultaneously, each logging to different Flurry accounts.  Therefore, apps that use the Affdex SDK for Android cannot also use the Flurry Analytics service themselves, as doing so could result in the app's analytics events being logged to the Affectiva Flurry account, or vice versa.  We are exploring ways to address this limitation in future releases. <br>
Where to go from here <br>
We’re excited to help you get the most of our SDK in your application. Please use the following ways to contact us with questions, comments, suggestions ... or even praise! <br>
Email: [sdk@affectiva.com] <br>
[http://www.affdex.com/mobile-sdk]  </p>